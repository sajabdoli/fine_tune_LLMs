{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJxHlUDT0Upl0lzgv6AtnG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajabdoli/fine_tune_LLMs/blob/main/DPO_fine_tune_LLMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install trl"
      ],
      "metadata": {
        "id": "lVwCx6I0mA7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKZnXF6clBxC",
        "outputId": "1e0e79ea-4aaf-4215-ffa3-b38107ccb97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "5B6hZjmOlewh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset):\n",
        "    def format_dataset(samples):\n",
        "        return {\n",
        "            \"prompt\": samples[\"prompt\"],\n",
        "            \"chosen\": samples[\"chosen\"],\n",
        "            \"rejected\": samples[\"rejected\"]\n",
        "        }\n",
        "    return dataset.map(format_dataset)"
      ],
      "metadata": {
        "id": "JnxA2dyJoI96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_lora(model):\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,  # Rank of the update matrices\n",
        "        lora_alpha=32,  # Scaling factor for LoRA updates\n",
        "        target_modules=[\n",
        "            # Target all linear layers typically found in transformer models\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\",  # Query, Key, Value projections\n",
        "            \"o_proj\",  # Output projection\n",
        "            \"gate_proj\", \"down_proj\", \"up_proj\",  # MLP layers\n",
        "            \"embed_tokens\",  # Embedding layer\n",
        "            \"lm_head\"  # Language model head\n",
        "        ],\n",
        "        lora_dropout=0.1,  # Dropout rate for LoRA layers\n",
        "        bias=\"none\",  # No bias adaptation\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Prepare model for low-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Apply LoRA configuration\n",
        "    return get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "OqCDAiyToMSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    load_in_8bit=True,  # Enable 8-bit quantization\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "KBhYzyfuoXwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess dataset\n",
        "dataset = load_dataset(\"ayoubkirouane/Orca-Direct-Preference-Optimization\")\n",
        "processed_dataset = prepare_dataset(dataset)\n",
        "\n",
        "# Prepare model with LoRA\n",
        "model = configure_lora(model)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPJkKvqcqGlE",
        "outputId": "6e3e4997-cf1b-4d90-c6a7-b109975712a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,792,320 || all params: 2,784,476,160 || trainable%: 0.1721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = DPOConfig(\n",
        "    output_dir=\"./dpo_model\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "    max_steps=200,\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    max_grad_norm=0.3\n",
        ")"
      ],
      "metadata": {
        "id": "ePGEFGFOpHrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,  # Optional: you can pass a reference model if needed\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=256,  # Add max sequence length\n",
        "    max_prompt_length=128,  # Limit prompt length\n",
        "    max_target_length=128  # Limit target length\n",
        ")\n"
      ],
      "metadata": {
        "id": "MlgLRvYZRUDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "dpo_trainer.train()"
      ],
      "metadata": {
        "id": "WTnB7qVeRZFg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}